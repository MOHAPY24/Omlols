
# **Omlols (OhMyLlamaOnLifeSupport)**

### *An Ollama custom client for prompt-engineering and modularity*

#### *Built and used personally by me (momo)!*

---

## ğŸš€ **What is Omlols?**

**Omlols** â€” short for **OhMyLlamaOnLifeSupport** â€” is a lightweight, customizable Python client designed to interact with **Ollama** in a clean, modular, and prompt-engineer-friendly way.

This project is built for personal use, but it's structured well enough for others to extend, study, or borrow ideas from. The main goal is to:

* Create a **clean interactive interface** for Ollama
* Allow **deep prompt control** and **prompt-engineering experiments**
* Provide **modularity** for plugins, memory, and external text sources
* Keep everything simple, readable, and easy to modify

If you're into tinkering with LLM clients, RAG experiments, or custom workflowsâ€”Omills is for you.

---

## ğŸ“¦ **Features**

* ğŸ§© Modular plugin system
* ğŸ§  Optional chat memory integration
* ğŸ“š External text source loading
* âœ¨ Clean prompt construction
* âš™ï¸ Built for experimenters and prompt engineers
* ğŸ’» Fully Python-based

---

## ğŸ“‹ **Prerequisites**

Make sure you have the following installed:

1. **Python 3.9+**
2. **Ollama** (installed and running)
3. **tinyllama** model downloaded in Ollama
4. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ”§ **Installation & Usage**

Clone the repo:

```bash
git clone https://github.com/MOHAPY24/Omlols
cd Omills
```

Run Omills:

```bash
chmod +x omlols
./omlols
```

Customize your plugins, memory behavior, and sources as you like â€” the code is intentionally easy to modify.

---

## ğŸ“„ **License**

Omlols (OhMyLlamaOnLifeSupport) is licensed under the **Apache 2.0 License**, with attribution provided in the [NOTICE](NOTICE) file.
See the full license in [LICENSE](LICENSE).

